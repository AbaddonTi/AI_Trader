import torch, ccxt, time, ta
import torch.nn as nn
import shutil
import numpy as np
import pandas as pd
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import MinMaxScaler
from datetime import datetime
from tqdm import tqdm
import os

lines_to_remove = 2160  # например, удалить последние 10 строк 

class TimeSeriesDataset(torch.utils.data.Dataset):
    def __init__(self, X, y):
        super(TimeSeriesDataset, self).__init__()
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

class GLU(nn.Module):
    def __init__(self, input_size, output_size):
        super(GLU, self).__init__()
        self.fc_linear = nn.Linear(input_size, output_size)
        self.fc_gates = nn.Linear(input_size, output_size)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        linear = self.fc_linear(x)
        gates = self.sigmoid(self.fc_gates(x))
        return linear * gates

class MultiheadSelfAttention(nn.Module):

    def __init__(self, d_model, num_heads):
        super(MultiheadSelfAttention, self).__init__()
        # «Количество головок должно быть фактором размера модели»
        assert d_model % num_heads == 0, "Number of heads must be a factor of the model dimension"

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        self.fc_query = nn.Linear(d_model, d_model)
        self.fc_key = nn.Linear(d_model, d_model)
        self.fc_value = nn.Linear(d_model, d_model)
        self.fc_out = nn.Linear(d_model, d_model)

        self.dropout = nn.Dropout(0.1)
        self.softmax = nn.Softmax(dim=-1)

    def split_heads(self, x):
        batch_size, seq_len, _ = x.size()
        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)

    def combine_heads(self, x):
        batch_size, _, seq_len, _ = x.size()
        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)

    def forward(self, x):
        query = self.split_heads(self.fc_query(x))
        key = self.split_heads(self.fc_key(x))
        value = self.split_heads(self.fc_value(x))

        # Вычислить скалярное произведение внимания (в масштабе)
        scores = torch.matmul(query, key.transpose(-2, -1)) / (self.d_k ** 0.5)
        attn_weights = self.softmax(scores)
        attn_weights = self.dropout(attn_weights)
        context = torch.matmul(attn_weights, value)

        # Объединяем головы и проходим через выходной линейный слой
        context = self.combine_heads(context)
        output = self.fc_out(context)
        return output

class GatedResidualNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.1):
        super(GatedResidualNetwork, self).__init__()

        self.fc_input = nn.Linear(input_size, hidden_size)
        self.fc_hidden = nn.Linear(hidden_size, output_size)
        self.fc_gates_input = nn.Linear(input_size, output_size)
        self.fc_gates_hidden = nn.Linear(hidden_size, output_size)

        self.relu = nn.ReLU()
        self.glu = GLU(hidden_size, output_size)
        self.dropout = nn.Dropout(dropout_rate)
        self.layer_norm = nn.LayerNorm(output_size)

    def forward(self, x):
        input_x = x

        x = self.fc_input(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc_hidden(x)
        x = self.glu(x)

        gates = self.fc_gates_input(input_x) + self.fc_gates_hidden(x)
        x = input_x + self.dropout(gates * x)

        x = self.layer_norm(x)
        return x

class TemporalFusionTransformer(nn.Module):
    def __init__(self, num_inputs, num_outputs, d_model, num_heads, num_blocks, dropout_rate=0.1):
        super(TemporalFusionTransformer, self).__init__()

        self.num_inputs = num_inputs
        self.num_outputs = num_outputs
        self.d_model = d_model

        self.input_encoding = nn.Linear(num_inputs, d_model)

        self.attention_blocks = nn.ModuleList()
        self.grn_blocks = nn.ModuleList()

        for _ in range(num_blocks):
            self.attention_blocks.append(MultiheadSelfAttention(d_model, num_heads))
            self.grn_blocks.append(GatedResidualNetwork(d_model, d_model, d_model, dropout_rate))

        self.fc_out = nn.Linear(d_model, num_outputs)

    def forward(self, x):
        x = self.input_encoding(x)

        for attn_block, grn_block in zip(self.attention_blocks, self.grn_blocks):
            x_attn = attn_block(x)
            x = grn_block(x + x_attn)

        x = self.fc_out(x)
        return x
def train_tft_model(model, train_loader, val_loader, num_epochs, learning_rate, device):
    model.to(device)
    # Define the optimizer and loss function
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    criterion = nn.MSELoss(reduction='mean')

    for epoch in tqdm(range(num_epochs), desc="Training model", ncols=100):
        # Training phase
        model.train()
        train_loss = 0.0
        for i, (batch_x, batch_y) in enumerate(train_loader):
            # Send batch data to device
            batch_x = batch_x.to(device)
            batch_y = batch_y.to(device)
            # Forward pass
            outputs = model(batch_x)
            # Reshape batch_y to match outputs size
            batch_y = batch_y.unsqueeze(1).repeat(1, outputs.size(1), 1)
            # Compute loss and backward pass
            loss = criterion(outputs, batch_y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            # Accumulate training loss
            train_loss += loss.item()

        # Compute average training loss
        train_loss /= len(train_loader)

        # Validation phase
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                # Send batch data to device
                batch_x = batch_x.to(device)
                batch_y = batch_y.to(device)
                # Forward pass
                outputs = model(batch_x)
                # Reshape batch_y to match outputs size
                batch_y = batch_y.unsqueeze(1).repeat(1, outputs.size(1), 1)
                # Compute loss
                loss = criterion(outputs, batch_y)
                # Accumulate validation loss
                val_loss += loss.item()

        # Compute average validation loss
        val_loss /= len(val_loader)

        # Print losses for the current epoch
        print(f"Epoch {epoch + 1}/{num_epochs}: train_loss={train_loss:.6f}, val_loss={val_loss:.6f}")

    return model, val_loss

def fetch_complete_historical_data(symbol, timeframe):
    exchange = ccxt.binance({
        "rateLimit": 1200,
        "enableRateLimit": True,
    })

    all_candles = []
    limit = 1000
    since = exchange.parse8601('2015-01-01T00:00:00Z')
    current_date = datetime.now()
    until = int(current_date.timestamp() * 1000)  # в миллисекундах

    with tqdm(total=len(range(since, until, limit * exchange.parse_timeframe(timeframe) * 100)), desc="Fetching data", ncols=100) as pbar:
        while since < until:
            candles = exchange.fetch_ohlcv(symbol, timeframe, limit=limit, since=since)
            if not candles:
                break
            all_candles += candles
            since = candles[-1][0] + exchange.parse_timeframe(timeframe) * 100
            time.sleep(exchange.rateLimit / 1000)
            pbar.update(1)

    df = pd.DataFrame(all_candles, columns=["timestamp", "open", "high", "low", "close", "volume"])
    df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms")
    df.dropna(inplace=True)

    # Сохраняем в файл "Тикер_Таймфрейм_Датасоздания_History"
    creation_date = datetime.now().strftime("%Y%m%d")
    output_file = f"{symbol.split('/')[0]}_{timeframe}_{creation_date}_History.csv"
    df.to_csv(output_file, index=False)
    print(f"Saved historical data to: {output_file}")

    return output_file


def filter_date_ranges(df, date_ranges):
    """
    Фильтрация датафрейма на основе заданных интервалов дат.

    Parameters:
    - df: датафрейм, который нужно отфильтровать.
    - date_ranges: список кортежей, где каждый кортеж содержит начальную и конечную даты интервала.

    Returns:
    - отфильтрованный датафрейм.
    """
    mask = np.full(df.shape[0], False)  # начальная маска (все значения False)

    for start_date, end_date in date_ranges:
        current_mask = (df.index >= start_date) & (df.index <= end_date)
        mask = mask | current_mask

    return df[mask]

def remove_last_lines_from_csv(filename, lines_to_remove):
    with open(filename, 'r') as f:
        lines = f.readlines()
        lines = lines[:-lines_to_remove]

    with open(filename, 'w') as f:
        f.writelines(lines)

def remove_duplicates_from_csv(filename):
    df = pd.read_csv(filename)
    # Удаление дубликатов по столбцу timestamp (или другому уникальному столбцу)
    df.drop_duplicates(subset='timestamp', inplace=True)
    df.to_csv(filename, index=False)

def create_dataset(data, ):
    X, y = [], []
    n_steps_in, n_steps_out = 50, 5

    for i in range(len(data)):
        end_ix = i + n_steps_in
        out_end_ix = end_ix + n_steps_out

        if out_end_ix > len(data):
            break

        seq_X, seq_y = data[i:end_ix, :-1], data[end_ix:out_end_ix, -1]
        X.append(seq_X)
        y.append(seq_y)

    X = np.array(X)
    y = np.array(y)
    print(f"Created {len(X)} input/output sequences")
    return X, y

###############################################################################
def load_and_preprocess_data(filename):
    df = pd.read_csv(filename)
    df['timestamp'] = pd.to_datetime(df['timestamp'], format="%Y-%m-%d %H:%M:%S")
    df.set_index('timestamp', inplace=True)

    # Add technical indicators
    df = ta.add_all_ta_features(df, "open", "high", "low", "close", "volume", fillna=True)

    # Normalize the data
    scaler = MinMaxScaler()
    df[df.columns] = scaler.fit_transform(df[df.columns])

    min_val = scaler.data_min_[df.columns.get_loc("open")]
    max_val = scaler.data_max_[df.columns.get_loc("open")]

    df.to_csv("./df_1h_sol.csv", index=False)

    train_df, val_df, pred_df, c = df[-40000:], df[:7314], df[-100:], df[-10:]
    # Добавляем проверку типов
    print(type(pred_df))
    print(type(c))

    pred_df = pd.concat([pred_df, c])

    # Convert the data to PyTorch tensors
    train_X, train_y = create_dataset(train_df.values)
    val_X, val_y = create_dataset(val_df.values)
    pred_X, pred_y = create_dataset(pred_df.values)
    print(f"X shape: {train_X.shape}, X data type: {train_X.dtype}")
    print(f"y shape: {train_y.shape}, y data type: {train_y.dtype}")

    train_dataset = TimeSeriesDataset(train_X, train_y)
    val_dataset = TimeSeriesDataset(val_X, val_y)
    pred_dataset = TimeSeriesDataset(pred_X, pred_y)
    return train_dataset, val_dataset, pred_dataset, min_val, max_val

def generate_sliding_window(data, input_seq_len, output_seq_len):
    X = []
    y = []
    for i in range(len(data) - input_seq_len - output_seq_len):
        input_seq = data[i:i + input_seq_len]
        if all(input_seq.shape == input_seq[0].shape for input_seq in input_seq):
            X.append(np.array(input_seq))
            output_seq = data[i + input_seq_len:i + input_seq_len + output_seq_len][:, -1]
            y.append(np.array(output_seq))
    X = np.array(X)
    y = np.array(y)
    print("X shape:", X.shape, ", X data type:", X.dtype)
    print("y shape:", y.shape, ", y data type:", y.dtype)
    return X, y

def get_forecasts(model, data, device, min_val, max_val):
    model.eval()

    with torch.no_grad():
        loader = data

        predictions = []
        dates = []

        for batch_x, _ in loader:
            batch_x = batch_x.to(device)
            output = model(batch_x)
            pred = output.cpu().numpy().flatten()
            pred_val = denormalize(pred, min_val, max_val)

            # Сохраняем прогнозы и даты
            predictions.append(pred_val[-5:])
            current_date = datetime.now().strftime('%y%m%d%H')
            dates.append(current_date)

    return predictions, dates

def get_last_date_from_csv(filename):
    df = pd.read_csv(filename)
    last_date = df['timestamp'].iloc[-1]

    # Преобразование строки в объект datetime
    last_date_dt = datetime.strptime(last_date, "%Y-%m-%d %H:%M:%S")

    # Форматирование даты в требуемый формат
    formatted_date = last_date_dt.strftime('%y%m%d%H')

    return formatted_date

def save_to_excel(ticker, timeframe, predictions, dates, last_date, filename_prefix="forecast"):
    df = pd.DataFrame({
        'Date': [last_date for _ in predictions],
        'Predictions': [','.join(map(str, pred)) for pred in predictions]
    })
    filename = f"{filename_prefix}_{ticker}_{timeframe}_{len(df)}.xlsx"
    df.to_excel(filename, index=False, header=False)  # Здесь добавлено header=False
    print(f"Saved forecasts to: {filename}")

def denormalize(value, min_val, max_val):
    return value * (max_val - min_val) + min_val

if __name__ == "__main__":
    start_time = time.time()
    symbol = "BTC/USDT"
    timeframe = "1h"

    # Получение полной истории данных
    main_file = fetch_complete_historical_data(symbol, timeframe)

    # Загрузка данных в датафрейм и фильтрация по нужным датам
    df = pd.read_csv(main_file)
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df.set_index('timestamp', inplace=True)

    # Определение интервалов дат для сохранения
    date_ranges_to_keep = [('2019-09-30', '2020-10-26'), ('2022-06-06', datetime.now().strftime('%Y-%m-%d'))]

    # Фильтрация датафрейма на основе интервалов дат
    filtered_df = filter_date_ranges(df, date_ranges_to_keep)

    # Сохранение отфильтрованного датафрейма обратно в файл
    filtered_file = "filtered_" + main_file
    filtered_df.to_csv(filtered_file)

    while lines_to_remove > 0:
        # Создаем дубликат основного файла перед каждым проходом
        duplicated_file = "sol_historical_data_1h_last.csv"
        shutil.copy(filtered_file, duplicated_file)
        print(f"Created duplicated data: {duplicated_file}")

        # Удалите последние строки из дублированного файла
        remove_last_lines_from_csv(duplicated_file, lines_to_remove)
        lines_to_remove -= 1
        remove_duplicates_from_csv(duplicated_file)

        train, val, pred, min_val, max_val = load_and_preprocess_data(duplicated_file)

        # Создание входных и выходных последовательностей
        input_seq_len = 50
        output_seq_len = 5
        X_train, y_train = generate_sliding_window(train, input_seq_len, output_seq_len)
        X_val, y_val = generate_sliding_window(val, input_seq_len, output_seq_len)
        X_pred, y_pred = generate_sliding_window(pred, input_seq_len, output_seq_len)

        # Create DataLoader objects
        train_loader = DataLoader(train, batch_size=100, shuffle=True)
        val_loader = DataLoader(val, batch_size=100, shuffle=False)
        pred_loader = DataLoader(pred, batch_size=100, shuffle=False)

        # Set device
        device = torch.device("cuda")  # ("cuda")("cpu")

        num_inputs = train_loader.dataset.X.shape[-1]
        num_outputs = train_loader.dataset.y.shape[-1]
        d_model = 75
        num_heads = 3
        num_blocks = 3
        dropout_rate = 0.36
        learning_rate = 0.0008
        num_epochs = 10
        model = TemporalFusionTransformer(num_inputs, num_outputs, d_model, num_heads, num_blocks, dropout_rate)
        model, val_loss = train_tft_model(model, train_loader, val_loader, num_epochs, learning_rate, device)

        # Получение прогнозов и сохранение их в Excel
        predictions, dates = get_forecasts(model, pred_loader, device, min_val, max_val)
        last_date = get_last_date_from_csv(duplicated_file)

        # Если файл уже существует, вставляйте новые прогнозы в конец файла
        filename = f"forecast_{symbol.split('/')[0]}_{timeframe}_{len(predictions)}.xlsx"
        if os.path.exists(filename):
            # Загрузите существующие данные без заголовков
            existing_df = pd.read_excel(filename, header=None)

            new_df = pd.DataFrame({
                0: [last_date for _ in predictions],  # Изменим имя столбца на 0, так как мы не используем заголовки
                1: [','.join(map(str, pred)) for pred in predictions]  # Изменим имя столбца на 1
            })

            # Объедините существующий DataFrame и новый DataFrame по вертикали
            final_df = pd.concat([existing_df, new_df], ignore_index=True)

            # Сохраните объединенный DataFrame в файл Excel без заголовков
            final_df.to_excel(filename, index=False, header=False)
        else:
            save_to_excel(symbol.split('/')[0], timeframe, predictions, dates, last_date)

    end_time = time.time()  # Конец измерения времени
    print(f"Total execution time: {end_time - start_time:.2f} seconds.")
